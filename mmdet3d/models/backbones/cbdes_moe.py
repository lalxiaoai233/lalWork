"""
CBDES MoE: åˆ†å±‚è§£è€¦ä¸“å®¶æ··åˆæ¨¡å‹ç”¨äºè‡ªåŠ¨é©¾é©¶åŠŸèƒ½æ¨¡å—

è¿™ä¸ªæ¨¡å—å®ç°äº†CBDES MoEæ¶æ„ï¼ŒåŒ…å«å¼‚æ„ä¸“å®¶ç½‘ç»œå’Œè½»é‡çº§è‡ªæ³¨æ„åŠ›è·¯ç”±å™¨ï¼ˆSARï¼‰ç”¨äºåŠ¨æ€ä¸“å®¶é€‰æ‹©ã€‚

ä¸»è¦ç‰¹æ€§ï¼š
1. å››ç§å¼‚æ„ä¸“å®¶ç½‘ç»œï¼šSwin Transformerã€ResNetã€ConvNeXtã€PVT
2. è‡ªæ³¨æ„åŠ›è·¯ç”±å™¨ï¼ˆSARï¼‰è¿›è¡ŒåŠ¨æ€ä¸“å®¶é€‰æ‹©
3. è´Ÿè½½å‡è¡¡æ­£åˆ™åŒ–ç¡®ä¿ä¸“å®¶å‡åŒ€ä½¿ç”¨
4. æ”¯æŒImageNeté¢„è®­ç»ƒæƒé‡åŠ è½½
5. å¤šå°ºåº¦ç‰¹å¾è¾“å‡º

ä½œè€…ï¼šliuailin
"""

import math
import os
from typing import Dict, List, Optional, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
from mmcv.cnn import ConvModule
from mmcv.runner import BaseModule, load_checkpoint
from mmdet.models import BACKBONES
from mmdet.models.backbones import ResNet, SwinTransformer
from mmcv.cnn import build_norm_layer, build_activation_layer


def load_pretrained_weights(model, pretrained_path, strict=False):
    """
    ä¸ºä¸“å®¶ç½‘ç»œåŠ è½½é¢„è®­ç»ƒæƒé‡
    
    æ”¯æŒå¤šç§æ£€æŸ¥ç‚¹æ ¼å¼ï¼Œè‡ªåŠ¨è¿‡æ»¤åˆ†ç±»å¤´æƒé‡ï¼Œåªä¿ç•™éª¨å¹²ç½‘ç»œæƒé‡ã€‚
    è¿™æ˜¯ç¡®ä¿CBDES MoEèƒ½æ­£ç¡®åŠ è½½é¢„è®­ç»ƒä¸“å®¶ç½‘ç»œçš„å…³é”®å‡½æ•°ã€‚
    
    Args:
        model: è¦åŠ è½½æƒé‡çš„æ¨¡å‹
        pretrained_path: é¢„è®­ç»ƒæƒé‡æ–‡ä»¶è·¯å¾„
        strict: æ˜¯å¦ä¸¥æ ¼åŒ¹é…æ‰€æœ‰é”®å
        
    Returns:
        model: åŠ è½½æƒé‡åçš„æ¨¡å‹
    """
    # æ£€æŸ¥é¢„è®­ç»ƒæƒé‡è·¯å¾„æ˜¯å¦å­˜åœ¨
    if pretrained_path is None or not os.path.exists(pretrained_path):
        print(f"è­¦å‘Š: é¢„è®­ç»ƒæƒé‡æœªæ‰¾åˆ° {pretrained_path}")
        return model
    
    try:
        # åŠ è½½æ£€æŸ¥ç‚¹æ–‡ä»¶åˆ°CPUå†…å­˜
        checkpoint = torch.load(pretrained_path, map_location='cpu')
        
        # å¤„ç†ä¸åŒçš„æ£€æŸ¥ç‚¹æ ¼å¼
        if 'model' in checkpoint:
            state_dict = checkpoint['model']
        elif 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        else:
            state_dict = checkpoint
        
        # è¿‡æ»¤æ‰åˆ†ç±»å¤´æƒé‡ï¼Œåªä¿ç•™éª¨å¹²ç½‘ç»œæƒé‡
        filtered_state_dict = {}
        for key, value in state_dict.items():
            # è·³è¿‡åˆ†ç±»å¤´å±‚ï¼ˆåŒ…å«classifierã€headã€fcã€linearç­‰å…³é”®è¯ï¼‰
            if any(skip_key in key.lower() for skip_key in ['classifier', 'head', 'fc', 'linear']):
                continue
            filtered_state_dict[key] = value
        
        # åŠ è½½è¿‡æ»¤åçš„æƒé‡åˆ°æ¨¡å‹
        model.load_state_dict(filtered_state_dict, strict=strict)
        print(f"æˆåŠŸåŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrained_path}")
        
    except Exception as e:
        print(f"åŠ è½½é¢„è®­ç»ƒæƒé‡æ—¶å‡ºé”™: {e}")
        print("ç»§ç»­ä½¿ç”¨éšæœºåˆå§‹åŒ–...")
    
    return model


class ConvNeXtBlock(nn.Module):
    """
    ConvNeXtå—å®ç°ï¼Œç”¨äºä¸“å®¶ç½‘ç»œ
    
    ConvNeXtæ˜¯ç°ä»£å·ç§¯æ¶æ„ï¼Œç»“åˆäº†æ·±åº¦å¯åˆ†ç¦»å·ç§¯ã€å±‚å½’ä¸€åŒ–å’Œå±‚ç¼©æ”¾æŠ€æœ¯ã€‚
    è¿™ä¸ªå—æ˜¯ConvNeXtä¸“å®¶ç½‘ç»œçš„åŸºæœ¬æ„å»ºå•å…ƒã€‚
    """
    
    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):
        """
        åˆå§‹åŒ–ConvNeXtå—
        
        Args:
            dim: ç‰¹å¾ç»´åº¦
            drop_path: DropPathæ­£åˆ™åŒ–æ¦‚ç‡
            layer_scale_init_value: å±‚ç¼©æ”¾åˆå§‹åŒ–å€¼
        """
        super().__init__()
        # æ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼ˆ7x7å·ç§¯æ ¸ï¼‰
        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)
        # å±‚å½’ä¸€åŒ–
        self.norm = nn.LayerNorm(dim, eps=1e-6)
        # ç¬¬ä¸€ä¸ªç‚¹å·ç§¯ï¼ˆ1x1å·ç§¯çš„çº¿æ€§å±‚å½¢å¼ï¼‰
        self.pwconv1 = nn.Linear(dim, 4 * dim)  # 512
        # GELUæ¿€æ´»å‡½æ•°
        self.act = nn.GELU()
        # ç¬¬äºŒä¸ªç‚¹å·ç§¯
        self.pwconv2 = nn.Linear(4 * dim, dim)
        # å±‚ç¼©æ”¾å‚æ•°ï¼ˆå¯é€‰çš„æ®‹å·®ç¼©æ”¾ï¼‰
        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), 
                                requires_grad=True) if layer_scale_init_value > 0 else None
        # DropPathæ­£åˆ™åŒ–
        self.drop_path = nn.Identity() if drop_path == 0. else nn.Dropout(drop_path)

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ç‰¹å¾å¼ é‡ (B, C, H, W)
            
        Returns:
            x: è¾“å‡ºç‰¹å¾å¼ é‡ (B, C, H, W)
        """
        input = x
        # æ·±åº¦å¯åˆ†ç¦»å·ç§¯
        x = self.dwconv(x)
        # è½¬æ¢ç»´åº¦ç”¨äºå±‚å½’ä¸€åŒ– (N, C, H, W) -> (N, H, W, C)
        x = x.permute(0, 2, 3, 1)
        x = self.norm(x)
        # ç¬¬ä¸€ä¸ªç‚¹å·ç§¯å’Œæ¿€æ´»
        x = self.pwconv1(x)
        x = self.act(x)
        # ç¬¬äºŒä¸ªç‚¹å·ç§¯
        x = self.pwconv2(x)
        # åº”ç”¨å±‚ç¼©æ”¾
        if self.gamma is not None:
            x = self.gamma * x

        # è½¬æ¢å›åŸå§‹ç»´åº¦ (N, H, W, C) -> (N, C, H, W)
        x = x.permute(0, 3, 1, 2)

        # æ®‹å·®è¿æ¥
        x = input + self.drop_path(x)
        return x


class ConvNeXtExpert(nn.Module):
    """
    ConvNeXtä¸“å®¶ç½‘ç»œï¼Œæ”¯æŒé¢„è®­ç»ƒæƒé‡
    
    è¿™æ˜¯CBDES MoEä¸­çš„ConvNeXtä¸“å®¶ç½‘ç»œå®ç°ï¼Œæ”¯æŒä»torchvisionã€timmæˆ–è‡ªå®šä¹‰æ£€æŸ¥ç‚¹åŠ è½½é¢„è®­ç»ƒæƒé‡ã€‚
    ç½‘ç»œé‡‡ç”¨åˆ†å±‚ç»“æ„ï¼ŒåŒ…å«stemå±‚ã€å¤šä¸ªä¸‹é‡‡æ ·å±‚å’ŒConvNeXtå—ã€‚
    """
    
    def __init__(self, in_channels=3, depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], 
                 drop_path_rate=0., layer_scale_init_value=1e-6, out_indices=[1, 2, 3],
                 pretrained=None):
        """
        åˆå§‹åŒ–ConvNeXtä¸“å®¶ç½‘ç»œ
        
        Args:
            in_channels: è¾“å…¥é€šé“æ•°
            depths: æ¯ä¸ªé˜¶æ®µçš„å—æ•°é‡
            dims: æ¯ä¸ªé˜¶æ®µçš„ç‰¹å¾ç»´åº¦
            drop_path_rate: DropPathæ­£åˆ™åŒ–ç‡
            layer_scale_init_value: å±‚ç¼©æ”¾åˆå§‹åŒ–å€¼
            out_indices: è¾“å‡ºç‰¹å¾å›¾çš„ç´¢å¼•
            pretrained: é¢„è®­ç»ƒæƒé‡è·¯å¾„æˆ–'torchvision'/'timm'
        """
        super().__init__()
        self.out_indices = out_indices
        self.pretrained = pretrained
        
        # æ„å»ºstemå±‚ï¼ˆåˆå§‹ä¸‹é‡‡æ ·å±‚ï¼‰
        self.downsample_layers = nn.ModuleList()
        stem = nn.Sequential(
            nn.Conv2d(in_channels, dims[0], kernel_size=4, stride=4),  # 4x4å·ç§¯ï¼Œæ­¥é•¿4
            nn.BatchNorm2d(dims[0])
        )
        self.downsample_layers.append(stem)
        
        # æ„å»ºä¸‹é‡‡æ ·å±‚ï¼ˆé˜¶æ®µé—´çš„è¿‡æ¸¡å±‚ï¼‰
        for i in range(3):
            downsample_layer = nn.Sequential(
                nn.BatchNorm2d(dims[i]),
                nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),  # 2x2å·ç§¯ï¼Œæ­¥é•¿2
            )
            self.downsample_layers.append(downsample_layer)
        
        # æ„å»ºConvNeXtå—ï¼ˆæ¯ä¸ªé˜¶æ®µçš„æ ¸å¿ƒå¤„ç†å•å…ƒï¼‰
        self.stages = nn.ModuleList()
        # è®¡ç®—DropPathç‡ï¼ˆçº¿æ€§é€’å¢ï¼‰
        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]
        cur = 0
        for i in range(4):
            stage = nn.Sequential(
                *[ConvNeXtBlock(dim=dims[i], drop_path=dp_rates[cur + j],
                              layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]
            )
            self.stages.append(stage)
            cur += depths[i]
        
        # è¾“å‡ºå½’ä¸€åŒ–å±‚
        self.norm = nn.BatchNorm2d(dims[-1])
        
        # å¦‚æœæŒ‡å®šäº†é¢„è®­ç»ƒæƒé‡ï¼Œåˆ™åŠ è½½
        if self.pretrained:
            self._load_pretrained_weights()
    
    def _load_pretrained_weights(self):
        """Load pretrained ConvNeXt weights."""
        if self.pretrained == 'torchvision':
            # Use torchvision's pretrained ConvNeXt
            try:
                # Try different ConvNeXt model names
                if hasattr(models, 'convnext_tiny'):
                    pretrained_model = models.convnext_tiny(pretrained=True)
                elif hasattr(models, 'convnext_small'):
                    pretrained_model = models.convnext_small(pretrained=True)
                else:
                    print("ConvNeXt not available in this torchvision version, skipping pretrained weights")
                    print("ğŸ’¡ Solution: Upgrade torchvision to 0.13.0+ or use custom pretrained weights")
                    return
                
                # Extract backbone weights (exclude classifier)
                pretrained_state_dict = {}
                for name, param in pretrained_model.named_parameters():
                    if 'classifier' not in name:
                        pretrained_state_dict[name] = param.data
                
                # Load compatible weights
                model_state_dict = self.state_dict()
                for name, param in pretrained_state_dict.items():
                    if name in model_state_dict and param.shape == model_state_dict[name].shape:
                        model_state_dict[name] = param
                
                self.load_state_dict(model_state_dict, strict=False)
                print("Successfully loaded torchvision ConvNeXt pretrained weights")
            except Exception as e:
                print(f"Failed to load torchvision ConvNeXt weights: {e}")
        elif self.pretrained == 'timm':
            # Use timm library for ConvNeXt pretrained weights
            try:
                import timm
                pretrained_model = timm.create_model('convnext_tiny', pretrained=True)
                
                # Extract backbone weights (exclude classifier)
                pretrained_state_dict = {}
                for name, param in pretrained_model.named_parameters():
                    if 'head' not in name and 'classifier' not in name:
                        pretrained_state_dict[name] = param.data
                
                # Load compatible weights
                model_state_dict = self.state_dict()
                for name, param in pretrained_state_dict.items():
                    if name in model_state_dict and param.shape == model_state_dict[name].shape:
                        model_state_dict[name] = param
                
                self.load_state_dict(model_state_dict, strict=False)
                print("Successfully loaded timm ConvNeXt pretrained weights")
            except ImportError:
                print("timm library not available. Install with: pip install timm")
            except Exception as e:
                print(f"Failed to load timm ConvNeXt weights: {e}")
        else:
            # Load from custom checkpoint
            load_pretrained_weights(self, self.pretrained)
        
    def forward(self, x):
        outputs = []
        for i in range(4):
            x = self.downsample_layers[i](x)
            x = self.stages[i](x)
            if i in self.out_indices:
                outputs.append(x)
        
        return outputs


class PyramidVisionTransformerBlock(nn.Module):
    """Pyramid Vision Transformer Block."""
    
    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, 
                 drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=attn_drop, bias=qkv_bias)
        self.drop_path = nn.Identity() if drop_path == 0. else nn.Dropout(drop_path)
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = nn.Sequential(
            nn.Linear(dim, mlp_hidden_dim),
            act_layer(),
            nn.Dropout(drop),
            nn.Linear(mlp_hidden_dim, dim),
            nn.Dropout(drop)
        )

    def forward(self, x, H, W):
        B, N, C = x.shape
        x_norm = self.norm1(x)
        attn_out, _ = self.attn(x_norm, x_norm, x_norm)
        x = x + self.drop_path(attn_out)
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x


class PyramidVisionTransformerExpert(nn.Module):
    """Pyramid Vision Transformer Expert Network with pretrained support."""
    
    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dims=[64, 128, 320, 512],
                 num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=False, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,
                 depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1], out_indices=[1, 2, 3],
                 pretrained=None):
        super().__init__()
        self.out_indices = out_indices
        self.pretrained = pretrained
        
        # Patch embeddings
        self.patch_embeds = nn.ModuleList()
        self.pos_embeds = nn.ParameterList()
        self.pos_drops = nn.ModuleList()
        
        for i in range(len(embed_dims)):
            if i == 0:
                patch_embed = nn.Conv2d(in_channels, embed_dims[i], kernel_size=patch_size, stride=patch_size)
            else:
                patch_embed = nn.Conv2d(embed_dims[i-1], embed_dims[i], kernel_size=2, stride=2)
            self.patch_embeds.append(patch_embed)
            
            pos_embed = nn.Parameter(torch.zeros(1, (img_size // (patch_size * (2 ** i))) ** 2, embed_dims[i]))
            self.register_parameter(f'pos_embed_{i}', pos_embed)
            self.pos_embeds.append(pos_embed)
            self.pos_drops.append(nn.Dropout(p=drop_rate))
        
        # Transformer blocks
        self.blocks = nn.ModuleList()
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]
        cur = 0
        
        for i in range(len(embed_dims)):
            block = nn.ModuleList([
                PyramidVisionTransformerBlock(
                    dim=embed_dims[i], num_heads=num_heads[i], mlp_ratio=mlp_ratios[i],
                    qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate,
                    drop_path=dpr[cur + j], norm_layer=norm_layer
                ) for j in range(depths[i])
            ])
            self.blocks.append(block)
            cur += depths[i]
        
        self.norm = nn.LayerNorm(embed_dims[-1])
        
        # Load pretrained weights if specified
        if self.pretrained:
            self._load_pretrained_weights()
    
    def _load_pretrained_weights(self):
        """Load pretrained PVT weights."""
        if self.pretrained:
            load_pretrained_weights(self, self.pretrained)
        
    def forward(self, x):
        B = x.shape[0]
        outputs = []
        
        for i in range(len(self.patch_embeds)):
            x = self.patch_embeds[i](x)
            _, _, H, W = x.shape
            x = x.flatten(2).transpose(1, 2)  # B, N, C
            # Dynamic position encoding based on actual spatial dimensions
            pos_embed = self.pos_embeds[i]
            N = x.shape[1]
            if pos_embed.shape[1] != N:
                # interpolate absolute pos_embed to match current HxW
                N0, C = pos_embed.shape[1], pos_embed.shape[2]
                H0 = int(round(N0 ** 0.5))
                W0 = max(1, N0 // H0)
                pe = pos_embed[0].transpose(0, 1).reshape(C, H0, W0).unsqueeze(0)
                pe = F.interpolate(pe, size=(H, W), mode='bicubic', align_corners=False)
                pos_embed = pe[0].reshape(C, H * W).transpose(0, 1).unsqueeze(0)
            else:
                pos_embed = pos_embed[:, :N, :]
            x = x + pos_embed
            x = self.pos_drops[i](x)
            
            for blk in self.blocks[i]:
                x = blk(x, H, W)
            
            if i in self.out_indices:
                x_norm = x  # Skip normalization for now to avoid dimension issues
                x_norm = x_norm.transpose(1, 2).reshape(B, -1, H, W)
                outputs.append(x_norm)
            
            # Reshape back to 4D for next stage
            x = x.transpose(1, 2).reshape(B, -1, H, W)
        
        return outputs


class SelfAttentionRouter(nn.Module):
    """
    è½»é‡çº§è‡ªæ³¨æ„åŠ›è·¯ç”±å™¨ï¼ˆSARï¼‰ç”¨äºä¸“å®¶é€‰æ‹©
    
    è¿™æ˜¯CBDES MoEçš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£æ ¹æ®è¾“å…¥ç‰¹å¾åŠ¨æ€é€‰æ‹©æœ€åˆé€‚çš„ä¸“å®¶ç½‘ç»œã€‚
    é‡‡ç”¨ä¸‰æ­¥å·ç§¯æ± åŒ– + è‡ªæ³¨æ„åŠ›ç¼–ç  + ä¸“å®¶è¯„åˆ†çš„å®Œæ•´æµç¨‹ã€‚
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. ä¸‰æ­¥å·ç§¯å’Œæ± åŒ–å¤„ç†ï¼šé€æ­¥æå–å’Œå‹ç¼©å›¾åƒç‰¹å¾
    2. è‡ªæ³¨æ„åŠ›ç¼–ç ï¼šå°†ç‰¹å¾è½¬æ¢ä¸ºtokenåºåˆ—å¹¶è¿›è¡Œå¤šå¤´è‡ªæ³¨æ„åŠ›å¤„ç†
    3. å›¾åƒçº§åµŒå…¥ï¼šé€šè¿‡tokenå¹³å‡å¾—åˆ°å…¨å±€è¯­ä¹‰ä¿¡æ¯
    4. ä¸“å®¶è¯„åˆ†ï¼š3å±‚MLPè¾“å‡ºä¸“å®¶logitsï¼Œé€šè¿‡softmaxè½¬æ¢ä¸ºè·¯ç”±æ¦‚ç‡
    5. è´Ÿè½½å‡è¡¡æ­£åˆ™åŒ–ï¼šç¡®ä¿ä¸“å®¶å‡åŒ€ä½¿ç”¨
    
    å¤„ç†æµç¨‹ï¼š
    è¾“å…¥å›¾åƒ -> ä¸‰æ­¥å·ç§¯æ± åŒ– -> tokenåºåˆ— -> å¤šå¤´è‡ªæ³¨æ„åŠ› -> å›¾åƒçº§åµŒå…¥ -> ä¸“å®¶logits -> è·¯ç”±æ¦‚ç‡
    """
    
    def __init__(self, input_dim, num_experts=4, embedding_dim=128, num_heads=8, dropout=0.1):
        """
        åˆå§‹åŒ–è‡ªæ³¨æ„åŠ›è·¯ç”±å™¨
        
        Args:
            input_dim: è¾“å…¥ç‰¹å¾ç»´åº¦
            num_experts: ä¸“å®¶ç½‘ç»œæ•°é‡
            embedding_dim: åµŒå…¥ç»´åº¦
            num_heads: æ³¨æ„åŠ›å¤´æ•°ï¼ˆå½“å‰æœªä½¿ç”¨ï¼Œä¿ç•™æ¥å£ï¼‰
            dropout: Dropoutæ¦‚ç‡
        """
        super().__init__()
        self.num_experts = num_experts
        self.input_dim = input_dim
        
        # ä¸‰æ­¥å·ç§¯å’Œæ± åŒ–å¤„ç†æ¨¡å—
        self.conv_modules = nn.ModuleList()
        
        # ç¬¬ä¸€æ­¥ï¼š3x3å·ç§¯ + BN + PReLU + 2x2æœ€å¤§æ± åŒ–ï¼ˆæ­¥é•¿2ï¼‰
        self.conv_modules.append(nn.Sequential(
            nn.Conv2d(input_dim, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.PReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        ))
        
        # ç¬¬äºŒæ­¥ï¼š3x3å·ç§¯ + BN + PReLU + 2x2æœ€å¤§æ± åŒ–ï¼ˆæ­¥é•¿2ï¼‰
        self.conv_modules.append(nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.PReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        ))
        
        # ç¬¬ä¸‰æ­¥ï¼š3x3å·ç§¯ + BN + PReLU + 2x2æœ€å¤§æ± åŒ–ï¼ˆæ­¥é•¿2ï¼‰
        self.conv_modules.append(nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.PReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        ))
        
        # è‡ªæ³¨æ„åŠ›ç¼–ç æ¨¡å—
        self.multihead_attn = nn.MultiheadAttention(
            embed_dim=128,  # d_emb = 128
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )
        self.layer_norm = nn.LayerNorm(128)
        
        # ç¬¬ä¸‰æ­¥ï¼šä¸“å®¶è¯„åˆ†MLPï¼ˆ3å±‚å¸¦PReLUæ¿€æ´»ï¼‰
        self.expert_scorer = nn.Sequential(
            nn.Linear(128, 64),  # ç¬¬ä¸€å±‚ï¼š128 -> 64ï¼ˆç¬¬ä¸€æ¬¡å‹ç¼©ï¼‰
            nn.PReLU(),
            nn.Linear(64, 32),  # ç¬¬äºŒå±‚ï¼š64 -> 32ï¼ˆç¬¬äºŒæ¬¡å‹ç¼©ï¼‰
            nn.PReLU(),
            nn.Linear(32, num_experts),  # ç¬¬ä¸‰å±‚ï¼š32 -> 4ï¼ˆä¸“å®¶logitsï¼‰
        )
        
        # è´Ÿè½½å‡è¡¡æ­£åˆ™åŒ–ï¼šè·Ÿè¸ªä¸“å®¶ä½¿ç”¨æƒ…å†µ
        self.register_buffer('expert_counts', torch.zeros(num_experts))
        self.register_buffer('total_samples', torch.tensor(0))
        
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ï¼šé€šè¿‡ä¸‰æ­¥å·ç§¯æ± åŒ– + è‡ªæ³¨æ„åŠ›ç¼–ç  + ä¸“å®¶è¯„åˆ†è®¡ç®—è·¯ç”±æ¦‚ç‡å’ŒæŸå¤±
        
        å¤„ç†æµç¨‹ï¼š
        1. è¾“å…¥å›¾åƒç‰¹å¾å›¾ X (B, C, H, W)
        2. ä¸‰æ­¥å·ç§¯å’Œæ± åŒ–å¤„ç†ï¼šæ¯æ­¥åŒ…å«3x3å·ç§¯ã€BNã€PReLUæ¿€æ´»å’Œ2x2æœ€å¤§æ± åŒ–ï¼Œå¾—åˆ°ç‰¹å¾X3
        3. è‡ªæ³¨æ„åŠ›ç¼–ç ï¼š
           - å°†X3å±•å¹³æˆtokenåºåˆ—T (B, N, d_emb)ï¼Œå…¶ä¸­N=H*Wï¼Œd_emb=128
           - å¯¹Tè¿›è¡Œå¤šå¤´è‡ªæ³¨æ„åŠ›MHAå’Œå±‚å½’ä¸€åŒ–ï¼Œå¾—åˆ°T'
           - å¯¹T'çš„tokenç»´åº¦Nå–å¹³å‡ï¼Œå¾—åˆ°å›¾åƒçº§åµŒå…¥G (B, d_emb)
        4. ä¸“å®¶è¯„åˆ†ï¼š
           - å°†Gè¾“å…¥3å±‚å¸¦PReLUæ¿€æ´»çš„MLPï¼Œè¾“å‡ºä¸“å®¶logits S (B, num_experts)
           - é€šè¿‡softmaxå°†Sè½¬æ¢ä¸ºè·¯ç”±æ¦‚ç‡P (B, num_experts)
        5. è´Ÿè½½å‡è¡¡æŸå¤±è®¡ç®—
        
        Args:
            x: è¾“å…¥ç‰¹å¾å¼ é‡ (B, C, H, W)
            
        Returns:
            P: è·¯ç”±æ¦‚ç‡ (B, num_experts)ï¼Œè¡¨ç¤ºæ¯å¼ å›¾åƒåˆ†é…ç»™å„ä¸“å®¶çš„æ¦‚ç‡
            routing_loss: è´Ÿè½½å‡è¡¡æŸå¤±
        """
        B, C, H, W = x.shape
        
        # ç¬¬ä¸€æ­¥ï¼šä¸‰æ­¥å·ç§¯å’Œæ± åŒ–å¤„ç†ï¼Œå¾—åˆ°ç‰¹å¾X3
        x3 = x
        for conv_module in self.conv_modules:
            x3 = conv_module(x3)  # (B, 128, H', W')
        
        # ç¬¬äºŒæ­¥ï¼šè‡ªæ³¨æ„åŠ›ç¼–ç 
        B, C, H, W = x3.shape
        
        # å°†ç‰¹å¾X3å±•å¹³æˆtokenåºåˆ—T
        T = x3.flatten(2).transpose(1, 2)  # (B, N, d_emb) å…¶ä¸­ N = H*W, d_emb = 128
        
        # å¤šå¤´è‡ªæ³¨æ„åŠ›å¤„ç†
        T_attended, _ = self.multihead_attn(T, T, T)  # (B, N, d_emb)
        
        # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
        T_prime = self.layer_norm(T + T_attended)  # (B, N, d_emb)
        
        # å¯¹tokenç»´åº¦Nå–å¹³å‡ï¼Œå¾—åˆ°å›¾åƒçº§åµŒå…¥G
        G = T_prime.mean(dim=1)  # (B, d_emb) = (B, 128)
        
        # ç¬¬ä¸‰æ­¥ï¼šä¸“å®¶è¯„åˆ†ï¼ŒåŸºäºå›¾åƒçº§åµŒå…¥Gè®¡ç®—ä¸“å®¶logitså’Œè·¯ç”±æ¦‚ç‡
        S = self.expert_scorer(G)  # ä¸“å®¶logits (B, num_experts)
        P = F.softmax(S, dim=-1)  # è·¯ç”±æ¦‚ç‡ (B, num_experts)
        
        # è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±
        routing_loss = self._compute_load_balance_loss(P)
        
        return P, routing_loss
    
    def _compute_load_balance_loss(self, expert_weights):
        """
        è®¡ç®—è´Ÿè½½å‡è¡¡æ­£åˆ™åŒ–æŸå¤±
        
        é€šè¿‡ç›‘æ§ä¸“å®¶ä½¿ç”¨æƒ…å†µï¼Œç¡®ä¿æ‰€æœ‰ä¸“å®¶è¢«å‡åŒ€ä½¿ç”¨ï¼Œ
        é¿å…æŸäº›ä¸“å®¶è¢«è¿‡åº¦ä½¿ç”¨è€Œå…¶ä»–ä¸“å®¶è¢«å¿½ç•¥ã€‚
        
        Args:
            expert_weights: ä¸“å®¶æƒé‡ (B, num_experts)
            
        Returns:
            load_balance_loss: è´Ÿè½½å‡è¡¡æŸå¤±
        """
        if not self.training:
            return torch.tensor(0.0, device=expert_weights.device)
        
        # æ›´æ–°ä¸“å®¶ä½¿ç”¨è®¡æ•°
        expert_selections = torch.argmax(expert_weights, dim=-1)  # (B,) é€‰æ‹©æƒé‡æœ€å¤§çš„ä¸“å®¶
        for i in range(self.num_experts):
            self.expert_counts[i] += (expert_selections == i).sum().float()
        self.total_samples += expert_weights.shape[0]
        
        # è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±ï¼šä¸“å®¶ä½¿ç”¨æ¦‚ç‡çš„æ–¹å·®
        expert_probs = self.expert_counts / (self.total_samples + 1e-8)
        load_balance_loss = torch.var(expert_probs) * self.num_experts
        
        return load_balance_loss


@BACKBONES.register_module()
class CBDESMoE(nn.Module):
    """
    CBDES MoE: åˆ†å±‚è§£è€¦ä¸“å®¶æ··åˆæ¨¡å‹ç”¨äºBEVæ„ŸçŸ¥
    
    è¿™æ˜¯CBDES MoEçš„ä¸»è¦å®ç°ï¼Œé›†æˆäº†å¤šä¸ªç»“æ„å¼‚æ„çš„ä¸“å®¶ç½‘ç»œ
    å’Œè½»é‡çº§è‡ªæ³¨æ„åŠ›è·¯ç”±å™¨ï¼Œç”¨äºåŠ¨æ€ä¸“å®¶é€‰æ‹©ã€‚
    
    æ¶æ„ç‰¹ç‚¹ï¼š
    1. å››ç§å¼‚æ„ä¸“å®¶ç½‘ç»œï¼šSwin Transformerã€ResNetã€ConvNeXtã€PVT
    2. è‡ªæ³¨æ„åŠ›è·¯ç”±å™¨ï¼ˆSARï¼‰è¿›è¡Œæ™ºèƒ½ä¸“å®¶é€‰æ‹©
    3. è´Ÿè½½å‡è¡¡æ­£åˆ™åŒ–ç¡®ä¿ä¸“å®¶å‡åŒ€ä½¿ç”¨
    4. æ”¯æŒImageNeté¢„è®­ç»ƒæƒé‡åŠ è½½
    5. å¤šå°ºåº¦ç‰¹å¾è¾“å‡ºé€‚é…ä¸‹æ¸¸ä»»åŠ¡
    6. åŠ¨æ€è¾“å‡ºæŠ•å½±ç¡®ä¿ç»´åº¦ä¸€è‡´æ€§
    """
    
    def __init__(self, 
                 in_channels=3,
                 expert_configs=None,
                 router_config=None,
                 out_indices=[1, 2, 3],
                 pretrained_configs=None,
                 **kwargs):
        """
        åˆå§‹åŒ–CBDES MoEæ¨¡å‹
        
        Args:
            in_channels: è¾“å…¥å›¾åƒé€šé“æ•°
            expert_configs: ä¸“å®¶ç½‘ç»œé…ç½®å­—å…¸
            router_config: è·¯ç”±å™¨é…ç½®å­—å…¸
            out_indices: è¾“å‡ºç‰¹å¾å›¾ç´¢å¼•
            pretrained_configs: é¢„è®­ç»ƒæƒé‡é…ç½®
            **kwargs: å…¶ä»–å‚æ•°
        """
        super().__init__()
        
        self.out_indices = out_indices
        
        # é»˜è®¤é¢„è®­ç»ƒé…ç½®
        if pretrained_configs is None:
            pretrained_configs = {
                'swin': None,  # Swinæƒé‡æ–‡ä»¶è·¯å¾„
                'resnet': 'torchvision',  # ä½¿ç”¨torchvisioné¢„è®­ç»ƒResNet50
                'convnext': 'torchvision',  # ä½¿ç”¨torchvisioné¢„è®­ç»ƒConvNeXt
                'pvt': None  # PVTæƒé‡æ–‡ä»¶è·¯å¾„
            }
        
        # Default expert configurations
        if expert_configs is None:
            expert_configs = {
                'swin': {
                    'type': 'SwinTransformer',
                    'embed_dims': 96,
                    'depths': [2, 2, 6, 2],
                    'num_heads': [3, 6, 12, 24],
                    'window_size': 7,
                    'mlp_ratio': 4,
                    'qkv_bias': True,
                    'drop_rate': 0.,
                    'attn_drop_rate': 0.,
                    'drop_path_rate': 0.3,
                    'patch_norm': True,
                    'out_indices': out_indices,
                    'with_cp': False,
                    'convert_weights': True,
                    'pretrained': pretrained_configs['swin']
                },
                'resnet': {
                    'type': 'ResNet',
                    'depth': 50,
                    'num_stages': 4,
                    'out_indices': out_indices,
                    'norm_cfg': {'type': 'BN2d', 'requires_grad': True},
                    'norm_eval': False,
                    'pretrained': pretrained_configs['resnet']
                },
                'convnext': {
                    'type': 'ConvNeXtExpert',
                    'depths': [3, 3, 9, 3],
                    'dims': [96, 192, 384, 768],
                    'drop_path_rate': 0.,
                    'layer_scale_init_value': 1e-6,
                    'out_indices': out_indices,
                    'pretrained': pretrained_configs['convnext']
                },
                'pvt': {
                    'type': 'PyramidVisionTransformerExpert',
                    'img_size': 224,
                    'patch_size': 16,
                    'embed_dims': [64, 128, 320, 512],
                    'num_heads': [1, 2, 5, 8],
                    'mlp_ratios': [8, 8, 4, 4],
                    'qkv_bias': False,
                    'drop_rate': 0.,
                    'attn_drop_rate': 0.,
                    'drop_path_rate': 0.,
                    'depths': [3, 4, 6, 3],
                    'sr_ratios': [8, 4, 2, 1],
                    'out_indices': out_indices,
                    'pretrained': pretrained_configs['pvt']
                }
            }
        
        # Default router configuration
        if router_config is None:
            router_config = {
                'input_dim': 3,  # Input image channels
                'num_experts': 4,
                'embedding_dim': 128,
                'num_heads': 8,
                'dropout': 0.1
            }
        
        # Initialize expert networks
        self.experts = nn.ModuleDict()
        expert_names = ['swin', 'resnet', 'convnext', 'pvt']
        
        for name, config in expert_configs.items():
            # Extract pretrained parameter
            pretrained = config.pop('pretrained', None)
            
            # Remove 'type' field from config as it's not needed for constructor
            config = {k: v for k, v in config.items() if k != 'type'}
            
            if name == 'swin':
                expert = SwinTransformer(**config)
                if pretrained:
                    load_pretrained_weights(expert, pretrained)
                self.experts[name] = expert
            elif name == 'resnet':
                expert = ResNet(**config)
                if pretrained == 'torchvision':
                    # Load torchvision pretrained ResNet50
                    try:
                        pretrained_model = models.resnet50(pretrained=True)
                        # Extract backbone weights (exclude classifier)
                        pretrained_state_dict = {}
                        for name_param, param in pretrained_model.named_parameters():
                            if 'fc' not in name_param:  # Skip final classifier
                                pretrained_state_dict[name_param] = param.data
                        
                        # Load compatible weights
                        model_state_dict = expert.state_dict()
                        for name_param, param in pretrained_state_dict.items():
                            if name_param in model_state_dict and param.shape == model_state_dict[name_param].shape:
                                model_state_dict[name_param] = param
                        
                        expert.load_state_dict(model_state_dict, strict=False)
                        print("Successfully loaded torchvision ResNet50 pretrained weights")
                    except Exception as e:
                        print(f"Failed to load torchvision ResNet50 weights: {e}")
                elif pretrained:
                    load_pretrained_weights(expert, pretrained)
                self.experts[name] = expert
            elif name == 'convnext':
                self.experts[name] = ConvNeXtExpert(**config, pretrained=pretrained)
            elif name == 'pvt':
                self.experts[name] = PyramidVisionTransformerExpert(**config, pretrained=pretrained)
        
        # Initialize router
        self.router = SelfAttentionRouter(**router_config)
        
        # Output projection layers to ensure consistent output dimensions
        self.output_projections = nn.ModuleDict()
        self.target_dim = 256  # Target output dimension for all experts
    
    def init_weights(self):
        """
        åˆå§‹åŒ–æƒé‡
        
        ç”±äºä¸“å®¶ç½‘ç»œå·²ç»åŠ è½½äº†é¢„è®­ç»ƒæƒé‡ï¼Œè¿™ä¸ªæ–¹æ³•ä¸»è¦ç”¨äº
        åˆå§‹åŒ–è·¯ç”±å™¨å’Œå…¶ä»–æ–°æ·»åŠ çš„ç»„ä»¶ã€‚
        """
        # è·¯ç”±å™¨ä¼šä½¿ç”¨é»˜è®¤åˆå§‹åŒ–
        for module in self.router.modules():
            if isinstance(module, (nn.Conv2d, nn.Conv1d, nn.Linear)):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
        
    def forward(self, x):
        """
        CBDES MoEå‰å‘ä¼ æ’­
        
        è¿™æ˜¯CBDES MoEçš„æ ¸å¿ƒå‰å‘ä¼ æ’­è¿‡ç¨‹ï¼ŒåŒ…æ‹¬ï¼š
        1. é€šè¿‡è·¯ç”±å™¨è®¡ç®—ä¸“å®¶æƒé‡
        2. æ‰€æœ‰ä¸“å®¶ç½‘ç»œå¹¶è¡Œå¤„ç†è¾“å…¥
        3. åŠ¨æ€åˆ›å»ºè¾“å‡ºæŠ•å½±å±‚ç¡®ä¿ç»´åº¦ä¸€è‡´
        4. æ ¹æ®è·¯ç”±å™¨æƒé‡åŠ æƒç»„åˆä¸“å®¶è¾“å‡º
        
        Args:
            x: è¾“å…¥å¼ é‡ (B, C, H, W)
            
        Returns:
            final_outputs: å¤šå°ºåº¦ç‰¹å¾å›¾åˆ—è¡¨
            routing_loss: è´Ÿè½½å‡è¡¡æŸå¤±
        """
        # ä»è·¯ç”±å™¨è·å–ä¸“å®¶é€‰æ‹©æƒé‡
        expert_weights, routing_loss = self.router(x)  # (B, num_experts)
        
        # ç¡®ä¿è·¯ç”±æƒé‡æ˜¯æœ‰æ•ˆçš„ï¼ˆé¿å…å…¨0æˆ–NaNï¼‰
        if torch.isnan(expert_weights).any() or (expert_weights.sum(dim=-1) < 1e-6).any():
            # å¦‚æœè·¯ç”±æƒé‡æœ‰é—®é¢˜ï¼Œä½¿ç”¨å‡åŒ€æƒé‡
            expert_weights = torch.ones_like(expert_weights) / self.router.num_experts
        
        # é€šè¿‡æ‰€æœ‰ä¸“å®¶ç½‘ç»œè¿›è¡Œå‰å‘ä¼ æ’­
        expert_outputs = {}
        expert_names = list(self.experts.keys())
        
        for name, expert in self.experts.items():
            expert_outputs[name] = expert(x)
        
        # ç¡®ä¿ä¸“å®¶è¾“å‡ºæ˜¯åˆ—è¡¨æ ¼å¼ï¼Œå¹¶ä¸”é•¿åº¦æ­£ç¡®
        for name in expert_names:
            if not isinstance(expert_outputs[name], (list, tuple)):
                expert_outputs[name] = [expert_outputs[name]]
            # ç¡®ä¿è¾“å‡ºæ•°é‡ä¸out_indicesåŒ¹é…
            if len(expert_outputs[name]) != len(self.out_indices):
                raise ValueError(f"Expert {name} output length {len(expert_outputs[name])} "
                               f"does not match out_indices length {len(self.out_indices)}")
        
        # åŠ¨æ€åˆ›å»ºè¾“å‡ºæŠ•å½±å±‚ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        for name in expert_names:
            if name not in self.output_projections:
                self.output_projections[name] = nn.ModuleList()
                for i in range(len(self.out_indices)):
                    # è·å–ä¸“å®¶çš„å®é™…è¾“å‡ºç»´åº¦
                    actual_dim = expert_outputs[name][i].shape[1]
                    # ä»…åšé€šé“å¯¹é½ï¼Œä¿æŒåŸå§‹ç©ºé—´å°ºå¯¸ä¸ vtransform æœŸæœ›ä¸€è‡´
                    projection = nn.Sequential(
                        nn.Conv2d(actual_dim, self.target_dim, 1)
                    )
                    self.output_projections[name].append(projection)
                    # ç§»åŠ¨åˆ°ä¸ä¸“å®¶è¾“å‡ºç›¸åŒçš„è®¾å¤‡
                    projection.to(expert_outputs[name][i].device)
        
        # åº”ç”¨è¾“å‡ºæŠ•å½±ç¡®ä¿é€šé“ä¸€è‡´ï¼ˆç©ºé—´å°ºå¯¸å…ˆä¸å˜ï¼‰
        projected_outputs = {}
        for name in expert_names:
            projected_outputs[name] = []
            for i in range(len(self.out_indices)):
                projected_feat = self.output_projections[name][i](expert_outputs[name][i])
                projected_outputs[name].append(projected_feat)
        
        # æ ¹æ®è·¯ç”±å™¨æƒé‡åŠ æƒç»„åˆä¸“å®¶è¾“å‡º
        final_outputs = []
        for i in range(len(self.out_indices)):
            # å¯¹ç¬¬ i ä¸ªå°ºåº¦ï¼Œå…ˆå¯¹é½æ‰€æœ‰ä¸“å®¶çš„ç©ºé—´åˆ†è¾¨ç‡åˆ°ç»Ÿä¸€ç›®æ ‡
            # é€‰æ‹©ç¬¬ä¸€ä¸ªä¸“å®¶çš„å°ºå¯¸ä½œä¸ºç›®æ ‡ï¼Œæˆ–é€‰æ‹©æœ€å¤§å°ºå¯¸
            target_H, target_W = None, None
            for j, name in enumerate(expert_names):
                H, W = projected_outputs[name][i].shape[-2:]
                if target_H is None:
                    target_H, target_W = H, W
                else:
                    if H * W > target_H * target_W:
                        target_H, target_W = H, W

            # è¿›è¡Œç©ºé—´å¯¹é½
            aligned_feats = []
            for j, name in enumerate(expert_names):
                feat = projected_outputs[name][i]
                if feat.shape[-2:] != (target_H, target_W):
                    feat = F.interpolate(feat, size=(target_H, target_W), mode='bilinear', align_corners=False)
                aligned_feats.append(feat)

            weighted_features = None
            for j, name in enumerate(expert_names):
                expert_feat = aligned_feats[j]
                # æ‰©å±•æƒé‡ç»´åº¦ä»¥åŒ¹é…ç‰¹å¾å›¾ç»´åº¦
                weight = expert_weights[:, j:j+1].unsqueeze(-1).unsqueeze(-1)
                
                if weighted_features is None:
                    weighted_features = weight * expert_feat
                else:
                    weighted_features += weight * expert_feat
            
            # ç¡®ä¿åŠ æƒåçš„ç‰¹å¾ä¸æ˜¯NaNæˆ–Inf
            if torch.isnan(weighted_features).any() or torch.isinf(weighted_features).any():
                # å¦‚æœç‰¹å¾æœ‰é—®é¢˜ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªä¸“å®¶çš„ç‰¹å¾
                weighted_features = aligned_feats[0]
            
            final_outputs.append(weighted_features)
        
        # ä¿å­˜è·¯ç”±æŸå¤±ä¾›ä¸Šå±‚æ¨¡å‹è¯»å–ï¼ˆå¦‚éœ€è¦ï¼‰
        self.routing_loss = routing_loss
        # è¿”å›å¤šå°ºåº¦ç‰¹å¾å’Œè·¯ç”±æŸå¤±ï¼Œæ»¡è¶³CBDESBEVFusionçš„è¾“å…¥çº¦å®š
        return final_outputs, routing_loss
    
    def get_expert_utilization(self):
        """
        è·å–ä¸“å®¶åˆ©ç”¨ç‡ç»Ÿè®¡ä¿¡æ¯
        
        è¿”å›æ¯ä¸ªä¸“å®¶ç½‘ç»œçš„ä½¿ç”¨æƒ…å†µç»Ÿè®¡ï¼Œç”¨äºç›‘æ§è´Ÿè½½å‡è¡¡æ•ˆæœ
        å’Œæ¨¡å‹è®­ç»ƒçŠ¶æ€åˆ†æã€‚
        
        Returns:
            dict: ä¸“å®¶åˆ©ç”¨ç‡å­—å…¸ï¼Œé”®ä¸º'expert_i'ï¼Œå€¼ä¸ºåˆ©ç”¨ç‡æ¯”ä¾‹
        """
        if hasattr(self.router, 'expert_counts'):
            total = self.router.total_samples.item()
            if total > 0:
                utilization = self.router.expert_counts / total
                return {f'expert_{i}': utilization[i].item() for i in range(len(utilization))}
        return {}
